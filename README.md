# A.R.E.S â€“ Artificial Reasoning Engineered System

Ares is a **fully local voice assistant** that combines:
- ğŸ¤ **Whisper.cpp** for speech-to-text (STT)
- ğŸ§  **Llama.cpp** for natural language processing (LLM)
- ğŸ”Š **Piper** for text-to-speech (TTS)
- ğŸ‘‚ **OpenWakeWord** for wake word detection ("Hey Ares")

Everything runs **offline** â€” no internet is required for processing.  
Ares is designed to be modular, hackable, and extendable to control smart devices or even robots.

---

## âœ¨ Features (Current Progress)

âœ… **Wake Word ("Hey Ares/Ares")**  
- Powered by [OpenWakeWord](https://github.com/dscripka/openWakeWord?tab=readme-ov-file)  
- Starts listening only after hearing the wake phrase  

âœ… **Speech-to-Text (STT)**  
- Uses `sounddevice` + `webrtcvad` for smart recording (stops when you go quiet)  
- Transcribes audio with `whisper-cli` (from Whisper.cpp)  

âœ… **Local Language Model (LLM)**  
- Runs `llama.cpp` in server mode  
- Configurable system prompt â†’ "Jarvis"-like personality  

âœ… **Text-to-Speech (TTS)**  
- Piper HTTP server generates natural-sounding voices  
- Multiple voices available (e.g., `en_US-bryce-medium`)  

âœ… **Main Pipeline**  
Wake Word â†’ Record â†’ Transcribe â†’ Send to LLM â†’ Speak Response

âœ… **Benchmarking**  
- `benchmark_ai.sh` logs timings for STT, LLM, and TTS  
- Results are stored in `latency.md`  

âœ… **CI / Mock Mode**  
- GitHub Actions run Ares in **Mock Mode** (no audio hardware required)  
- Simulates STT, LLM, and TTS responses for automated testing  

---

## Getting Started

### 1. Install dependencies
```bash
pip install -r requirements.txt
```

Also build:
- [Whisper.cpp](https://github.com/ggml-org/whisper.cpp)
- [Llama.cpp](https://github.com/ggml-org/llama.cpp)
- [Piper](https://github.com/rhasspy/piper)


### 2. Start LLM + TTS servers
```bash
./scripts/run_servers.sh
```

Say "Hey Ares", wait for the beep ğŸµ, then speak your command.
Ares will listen, process locally, and respond with speech.

## ğŸ§ª Development & Testing
### Benchmark Latency
```bash
./scripts/benchmark_ai.sh
```

## ğŸ“‚ Project Structure
```graphql
llmio/               # Input/output modules
 â”œâ”€ stt_whisper.py   # Speech-to-text
 â”œâ”€ tts_piper.py     # Text-to-speech
 â”œâ”€ llm_remote.py    # LLM client
 â””â”€ wake_word.py     # Wake word listener

scripts/             # Helper scripts
 â””â”€ run_servers.sh   # Start LLM and TTS servers

latency.md           # Benchmark results
ci_runner.py         # CI harness with mocks
main.py              # Main application loop

```

## âš™ï¸ Roadmap

### Voice & Interaction
- Custom wake word â€” trainable per device/user.
- Custom voice â€” selectable TTS voice profiles.

### Web & App Actions
- Open websites & apps on command

### Devices & I/O
- Bluetooth device control â€” pair/connect/disconnect and volume controls.

### Perception
- Visual detection â€” optional camera input for object/face/basic scene cues.

### Architecture/Security
- Speaker recognition â€” per-user profiles for personalization/permissions.
- Split LLM server â€” dedicated local endpoint with resource limits & auth.

### Hardware
- Custom hardware build â€” mic array, LEDs, physical mute, action button.
